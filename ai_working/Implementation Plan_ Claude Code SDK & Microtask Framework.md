# Implementation Plan: Claude Code SDK & Microtask Framework

**_(Complement to the previously outlined concept – this document assumes readers have first read the high-level design and microtask approach.)_**

## Project Overview and Goals

The goal is to create a **Claude Code SDK with a microtask-driven framework** that encapsulates Amplifier’s successful patterns into a standalone tool environment. This will allow a small dev team to implement a first-pass system that **mirrors key Amplifier capabilities** – but in a self-contained project, separate from the main Amplifier codebase. We will leverage the _learnings from Amplifier’s Claude Code integration_ (e.g. specialized agents, knowledge integration, automated quality checks) and push them down into this new SDK.

**Key Objectives for the First Iteration:**

*   **Reproduce Crucial Amplifier Features:** Include the foundational pieces that made Amplifier powerful – e.g. specialized agent roles, pre-loaded contextual knowledge, a knowledge base, and automation tools for quality checks.
*   **Microtask Orchestration:** Implement a _microtask-driven workflow_ where complex user requests are broken into smaller, dependable subtasks (as defined in the concept doc). This ensures the AI handles multi-step tasks reliably (e.g. _plan → execute → verify_ loops)[\[1\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Recipe%20sharing%20and%20discovery).
*   **Full Encapsulation in a New Project:** Build this as a new project/repository (or set of repos) independent of Amplifier’s codebase. All necessary context, agent definitions, config, and hooks will reside here, eliminating co-mingling with Amplifier’s resources. (We will **copy or adapt** any valuable components from Amplifier as needed while writing new code for the SDK’s specific needs.)
*   **Functional MVP:** Scope the implementation to something a small team can deliver quickly – it should **work end-to-end for at least one representative workflow** (for example, a code generation task with planning and testing, or a document analysis with triage and synthesis). Aim for a minimally robust system that demonstrates the approach, even if not all advanced features are in place yet. We will prioritize core functionality and proven patterns (“table stakes” features) to ensure robustness, and defer more complex enhancements to future iterations.

By focusing on these goals, the first pass will yield a working microtask-oriented development assistant, akin to a slimmed-down Amplifier, that can be extended over time.

## Architecture and Key Components

To implement the above, we propose a **modular architecture** composed of several clear components. This design will mirror Amplifier’s philosophy of _self-contained “bricks” with clear interfaces_[\[2\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Modular%20Architecture). Each piece does one thing well, making the system easier to develop, test, and extend. Below are the major components and how they relate to Amplifier’s existing systems:

### 1\. **Microtask Orchestrator (Task Manager)**

This is the heart of the new SDK – a controller that manages the breakdown of a high-level goal into microtasks and coordinates their execution. Its responsibilities include:

*   **Defining Task Sequences (“Recipes”):** It should be able to load or define a sequence of sub-tasks (a _recipe_) appropriate for the user’s request. For example, a code-generation recipe might be: _Plan → Implement → Test → Refine_. A research/analysis recipe might be: _Triage → DeepAnalyze → Synthesize_. (These recipes correspond to the _microtask-driven approach_ outlined in the concept doc, and to Amplifier’s emerging “recipes” concept[\[1\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Recipe%20sharing%20and%20discovery).)
*   **Managing Workflow State:** The orchestrator will carry the state between steps. It should collect outputs from each microtask and feed them as context into the next. This ensures continuity and that each sub-task builds on prior results.
*   **Ensuring Dependable Execution:** The framework should make each step **deterministic and check-pointed**. For instance, after code generation, the orchestrator can automatically run tests and decide if another debugging task is needed (i.e. a feedback loop until success criteria are met). This addresses the need for a _“dependable execution framework”_[\[1\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Recipe%20sharing%20and%20discovery) – each step either succeeds or triggers error-handling sub-tasks.
*   **Interface for Human or System Triggers:** In the future, the orchestrator can allow users or system rules to trigger particular recipes. (Amplifier, for example, let users invoke specialized agents via commands like /ultrathink-task or automatically chose TRIAGE mode for large data[\[3\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=%F0%9F%94%8D%20TRIAGE%20Mode%20,Relevance%20Filtering). In our first pass, we can keep it simpler – automatically choose a fixed recipe based on the task type, or even just focus on one primary workflow.)
*   **Example:** If a user asks: _“Please implement feature X and ensure it’s well-tested,”_ the orchestrator might execute the “Implement Feature” recipe:
*   **Planning Task:** Calls an agent to outline the solution approach (e.g. design and steps).
*   **Coding Task:** Calls an agent to write the code for the feature.
*   **Testing Task:** Runs automated tests or uses an agent to generate/run tests.
*   **Review Task:** If tests failed, loop back to debugging (invoke a debugging agent to fix issues), then rerun tests.
*   **Completion:** Once verification passes, present the final code/summary to the user.

Each of these are microtasks that the orchestrator handles in sequence, calling the appropriate agent and tools for each.

**Why this matters:** By centralizing control, we ensure the AI doesn’t try to juggle everything in one go. Instead, it tackles one well-defined sub-problem at a time, yielding more reliable results. This design aligns with Amplifier’s notion that _complex tasks can be tackled with minimal guidance by drawing from patterns and knowledge base_[\[4\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,the%20AI%20through%20complex%20tasks) – the orchestrator provides that guiding structure so the AI can focus on each step.

### 2\. **LLM Agent Interface**

This component handles all communication with the Claude model (or other future models). It is essentially a wrapper or SDK layer for calling the AI with certain prompts and getting responses. Key aspects:

*   **Model Abstraction:** We’ll encapsulate the specifics of calling Claude (Anthropic’s API/CLI) in one place. Today it will use Claude (the “Claude Code” model) as done in Amplifier, but tomorrow it might switch to GPT-5 or another model[\[5\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,to%20any%20particular%20AI%20technology). Designing an abstraction (like a ModelClient class or similar) allows swapping out the backend easily. (This follows Amplifier’s tech-agnostic approach: the value is in workflow and knowledge, not in any single AI model[\[6\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=The%20value%20of%20Amplifier%20isn%27t,specific%20AI%20%E2%80%94%20it%27s%20in).)
*   **Prompt Construction:** The interface should support constructing prompts for different contexts or “agents.” For instance, if the orchestrator says “invoke the testing agent,” the LLM interface can assemble a prompt that includes the testing agent’s instructions (from its profile, see Agent Definitions below), plus the relevant context (code to test, etc.). We may have a base system prompt containing Amplifier’s proven patterns and philosophies – for example, guidelines about clarity, not being sycophantic, focusing on simplicity, etc. Amplifier’s environment likely sets a default context with these philosophies[\[7\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Ruthless%20Simplicity), so we should do the same (ensuring each call to Claude includes these fundamental instructions).
*   **Maintaining Conversation Context:** Decide how to manage context across turns. Options:
*   _Single-turn prompts per microtask:_ Each sub-task prompt might stand alone (with necessary info explicitly provided by orchestrator). This avoids the model drifting off-track or exceeding context window, and it mirrors how discrete tasks are handled independently.
*   _Running conversational context:_ Alternatively, maintain a conversation history within each recipe so the agent remembers what was done in previous steps. Amplifier’s memory system can assist here (more below). For the first iteration, a pragmatic approach is to feed relevant pieces of previous steps as needed rather than the entire raw transcript (keeping prompts focused and within token limits).
*   **Claude Code Integration:** We should use the same integration method Amplifier uses for Claude. If Amplifier installed a “Claude CLI” tool, likely they call the Claude API via that. We have two choices:
*   Use the **Claude CLI**: Simplest might be to invoke the Claude CLI programmatically with the necessary prompt and get the output. This could be acceptable for a first pass (since Amplifier already has it set up, and it handles API keys, model selection, etc.).
*   Use **Anthropic’s API/SDK directly**: We could incorporate the official API calls (Anthropic provides Python SDK methods to call Claude). This would give more control (and avoid needing the CLI as a dependency). Either approach is fine – using what Amplifier already uses is acceptable as it’s proven to work. The key is to ensure the new tool can authenticate to Claude (e.g. via an API key, which in Amplifier is likely configured during the CLI install). We can mirror Amplifier’s practice: for example, ensure the devs set the Anthropic API key in the environment or a config file so the SDK can use it.
*   **Error Handling & Rate Limits:** The agent interface should handle common issues – e.g. API timeouts or refusals, and maybe automatically retry if safe. Also, if a response is incomplete or the AI misinterpreted instructions, the orchestrator (with help from this interface) might need to prompt again or clarify. Robustness here will improve reliability of each microtask execution.

### 3\. **Agent Definitions and Profiles**

Amplifier’s strength lies in its _“20+ specialized agents” – each an expert in specific tasks (architecture, debugging, security, etc.)._ We want to incorporate the essence of this by defining agent profiles in our SDK that encapsulate particular roles or skillsets. Key points:

*   **Agent Definition Format:** In Amplifier, these agents are defined in files (e.g. Markdown under .claude/agents/\*.md) with a name, description, examples, and specific instructions/tools for that role[\[8\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L4001%20%3Cfile%20path%3D%22.claude%2Fagents%2Fapi,design%2C%20review%2C%20or%20refactor%20API)[\[9\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=requires%20the%20api,Read%2C%20WebFetch%2C%20TodoWrite%2C%20WebSearch%2C%20BashOutput). We should adopt a similar approach:
*   Create a directory (e.g. agents/ or profiles/) containing definitions for each specialized agent we need.
*   Each definition includes: a **name**, a **description (when to use this agent)**, possibly example scenarios, and a set of **role instructions** (often phrased as if the assistant is told “You are a _X specialist_…”). It may also list **allowed tools/commands** that this agent can use (more on tools below).
*   These can be simple text/markdown or a structured format (YAML/JSON). Markdown is human-readable and was used in Amplifier, but a structured format might be easier for programmatic parsing. To start, we could stick with Markdown like Amplifier for familiarity, then parse as needed.
*   **Initial Agents to Include:** For the first iteration, we don’t need all 20+ from Amplifier. Focus on a handful that align with the microtask we want to implement. For example:
*   A **“Planner” or Architect agent** – to take a high-level request and break it into steps or design (e.g. planning code structure or outlining a solution).
*   A **“Coder” or Implementer agent** – to actually write code given a specification.
*   A **“Tester” agent** – to generate tests or analyze test results (or we could handle test execution outside the model, see Tools).
*   Optionally, a **“Debugger” agent** – to handle fixing code if tests fail or issues are found (this could be the same as the Coder agent but with a different prompt focusing on debugging).
*   For knowledge analysis tasks (if we include one): an **“Analysis” agent** that can synthesize information, possibly with modes like TRIAGE/DEEP/SYNTH. (Amplifier’s _analysis-engine_ or _content-researcher_ agents serve this purpose[\[10\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L5395%20,researcher%20agent%20to%20analyze%20our). We could include a simplified version if time permits.)
*   **Reuse Amplifier’s Content:** We can **copy/adapt some of Amplifier’s agent definitions** for these roles to jump-start. For instance:
*   The _api-contract-designer_, _content-researcher_, _concept-extractor_, etc., are defined in Amplifier’s .claude/agents/ folder[\[8\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L4001%20%3Cfile%20path%3D%22.claude%2Fagents%2Fapi,design%2C%20review%2C%20or%20refactor%20API)[\[9\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=requires%20the%20api,Read%2C%20WebFetch%2C%20TodoWrite%2C%20WebSearch%2C%20BashOutput). We can review those and extract the relevant parts for our chosen agents. The language in those files is a great starting point, since it’s been tuned (e.g. “You are an API contract design specialist…”[\[11\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=You%20are%20an%20API%20contract,needs%20rather%20than%20hypothetical%20futures) or the concept-extractor’s methodology).
*   For coding, Amplifier likely has an agent for implementation (maybe called something like “code-engineer” or they rely on Claude’s general coding ability with patterns). We might not have the exact file from Amplifier, but we can create a profile that emphasizes writing clean, modular code, following best practices, etc.
*   Keep the **philosophies** consistent: In the agent profiles (and/or base context) include Amplifier’s guiding principles for implementation (possibly referencing their _IMPLEMENTATION\_PHILOSOPHY.md_ if available). For example, _“Prefer simplicity over cleverness, avoid micro-optimizations that complicate code”_[\[12\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L10167%203.%20,maintenance%20cost%20of%20complex%20optimizations), or _“Focus on current requirements, not hypothetical future needs”_ as seen in the API contract agent[\[11\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=You%20are%20an%20API%20contract,needs%20rather%20than%20hypothetical%20futures). Embedding these ensures the AI agents behave in line with Amplifier’s ethos.
*   **Agent Selection:** Initially, we can have the **orchestrator explicitly choose the agent** for each step based on the recipe. (E.g. in the code implementation recipe: use Planner agent for step 1, Coder for step 2, etc.) This is straightforward to implement. Later on, we could make this dynamic – e.g., the orchestrator examines the user query and decides which specialized agent (or sequence of agents) is needed. Amplifier’s documentation shows examples of automatically picking an agent: _“the engine will select the optimal mode/agent based on context”_[\[10\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L5395%20,researcher%20agent%20to%20analyze%20our)[\[13\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=database%20performance%20issues.%5Cnuser%3A%20,architect). For now, a simple mapping from task type to agent suffices.
*   **Context Injection:** When invoking an agent, the LLM interface will inject that agent’s profile into the prompt (likely as a system or developer message) so that Claude adopts that persona/skillset. This is exactly how Amplifier’s Claude integration works: by starting Claude in the Amplifier directory, all enhancements (like agent definitions and context) are loaded automatically. We’ll replicate this by ensuring that when our SDK runs, it loads the agent definitions into memory or prompt context. In practice, this might mean concatenating a base system message that includes the relevant agent instructions prior to the user’s input for that step.

_Example:_ If the orchestrator is about to run the **Coding Task**, it would load the “Coder” agent profile. The prompt might look like:

_System_: “You are a senior software engineer tasked with implementing the solution as planned. Follow best practices (clean code, modular design, comments). If tests or specifications are provided, ensure the code meets them. Do not include extraneous output.” (This would be derived from the agent’s definition file.)
_Assistant_: (the prior plan content and specific coding instructions might also be included here by the orchestrator)
_User (implicitly the task instruction)_: “Implement the feature according to the plan.”

Then the Claude model, influenced by the system message, produces the code.

### 4\. **Knowledge Base & Memory System**

Robust assistance often requires **contextual knowledge** beyond the current conversation. Amplifier tackled this via a _Knowledge Extraction System_ that transforms documentation into queryable knowledge, plus a memory system for recalling past insights. For our SDK:

*   **Content Ingestion:** Provide a mechanism to ingest and index external content (documentation, project files, specs, etc.) so that agents can retrieve relevant information during tasks. We should replicate Amplifier’s approach:
*   Use external content directories (Amplifier uses an environment variable like AMPLIFIER\_CONTENT\_DIRS in .env to point to folders of notes, docs, etc). We can have a similar config (e.g. TOOL\_CONTENT\_DIRS) to list directories with knowledge sources.
*   Provide a script or function to **scan these directories and build a knowledge index**. In the first iteration, we can keep it simple: read text files (markdown, txt, maybe code files too) and store their contents in a searchable form.
*   Implement **semantic search with embeddings** if possible (Amplifier uses **sentence-transformers** for this[\[14\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L34698%20,Relevance%20scoring)). For example, integrate a lightweight embedding model (they used all-MiniLM-L6-v2[\[15\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Relevance%20scoring)) to vectorize content. If the dependency is too heavy, at least include it as optional (as Amplifier does – they fall back to keyword search if embeddings aren’t available[\[16\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Confidence%20scoring%20on%20edges)[\[17\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,search%20if%20embedding%20fails)). The dev team can likely copy Amplifier’s MemorySearcher or similar logic[\[18\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=query_embedding%20%3D%20self,type%3A%20ignore)[\[19\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=memory_embeddings%20%3D%20self,type%3A%20ignore), which handles encoding texts and performing cosine similarity search.
*   The result should be an ability to call something like knowledge\_search(query) and get the top relevant snippets or documents.
*   **Memory for Conversations:** In addition to static knowledge, maintaining _dynamic memory_ of what the AI has learned or decided in previous interactions can improve robustness:
*   Implement a **Memory Store** to save important facts or decisions from each session. Amplifier has a MemoryStore (JSON-based, with size limits and rotation)[\[20\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=class%20MemoryStore%3A%20%22%22%22JSON,with%20rotation%20and%20compatibility). We can adopt a simplified version: perhaps store each completed microtask’s key outputs as a “memory” that can be referenced. For example, after a planning task, store the plan so that the coding task can retrieve it without needing the full conversation text.
*   A **Memory Extractor** could automatically summarize or extract key points from a long discussion to add to memory[\[21\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=class%20MemoryExtractor%3A%20,conversation%20text), though this might be advanced for first pass. Alternatively, the orchestrator explicitly decides what to store (e.g., the final answer of a sub-agent could be stored with a label).
*   The **Memory Searcher** (as above) can then be used by the agent interface to pull any relevant memory when forming new prompts. For instance, if the user asks a follow-up next time, the system can surface past context or knowledge.
*   **Usage in Microtasks:** During a microtask, especially one involving analysis or coding in a larger project, the orchestrator or agent can query the knowledge base for relevant info:
*   For coding: If the user’s request references a specific API or library, a knowledge search might retrieve documentation or prior code examples from the content directory.
*   For analysis: If analyzing multiple documents, an initial _triage_ step could leverage a search index to pick the most relevant docs for detailed reading.
*   The orchestrator can automate some of this. For example, in a _Triage step_, instead of relying purely on the AI to scan 50 documents blindly, the orchestrator can pre-fetch summaries or relevant segments using the knowledge index and feed those to the agent. This approach was implied in Amplifier’s usage of modes (TRIAGE mode is likely implemented via quick filtering of docs)[\[22\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Initial%20Request%3A%20,SYNTHESIS%20mode%20to%20combine%20findings)[\[3\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=%F0%9F%94%8D%20TRIAGE%20Mode%20,Relevance%20Filtering).
*   **Data Directory:** We should follow Amplifier’s practice of using a dedicated data directory for storing all persistent data (memories, embeddings, etc). E.g., an environment variable for TOOL\_DATA\_DIR (akin to AMPLIFIER\_DATA\_DIR). All JSON files (knowledge index, memory store, etc.) would live there. This makes it easy to share or back up knowledge and avoids cluttering the project directory. The .env.example we create should prompt users to configure this if desired (or default to a local .data/ folder if not set).

By integrating a knowledge system, we ensure our tool’s AI has _grounding information and can learn over time_. This addresses the Amplifier value: _“The value isn’t the AI model, it’s the knowledge base, patterns, and workflows we’ve built”_[\[6\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=The%20value%20of%20Amplifier%20isn%27t,specific%20AI%20%E2%80%94%20it%27s%20in). Our SDK will carry over that value by making knowledge and memory first-class citizens.

### 5\. **Tools and Command Interfaces**

In complex tasks, the AI may need to perform actions beyond text generation – such as reading files, running code, searching the web, etc. Amplifier equips agents with a set of **automation tools** and enforces quality checks automatically. For our first pass, we will implement a subset of essential tools within the SDK, focusing on those needed for the primary use-case (and ensure the system can be extended with more tools later). Key tools we should consider:

*   **File System Tools:** Reading from and writing to files:
*   _Read files_: The agent may need to retrieve code from the project or open a documentation file. Implement a secure read\_file(path) that an agent or orchestrator can invoke to get file contents. (We should restrict paths to the project or content dirs to avoid arbitrary access.)
*   _List/Grep_: It can be useful to allow searching within the codebase. For example, if the user asks to modify a function but doesn’t specify where it is, an agent could use a grep tool to find occurrences. Amplifier lists tools like Glob, Grep, LS in agent definitions[\[23\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=requires%20the%20api,Read%2C%20WebFetch%2C%20TodoWrite%2C%20WebSearch%2C%20BashOutput). For simplicity, we can have an orchestrator step that does this (since implementing an interactive tool call parser is more complex). E.g., a microtask “Find relevant code” could run a grep under the hood and then feed results to the AI. If time permits, we can implement a lightweight command parser for the AI’s output so it can request these directly (perhaps prefixing with a special token or a JSON action format).
*   _Write output_: Eventually, if the AI writes code, we might let it create or modify files in a sandbox/worktree. In first iteration, we might not auto-write to the user’s actual project (to avoid destructive changes). Instead, we can output diffs or code blocks for the user. But having the capability to write to a temp directory is useful for running tests, etc. We can plan a write\_file(path, content) tool, and perhaps only execute it for test files or new code that the orchestrator then runs.
*   **Code Execution Tools:** Running code/tests in an isolated manner:
*   _Run tests or scripts_: To automate verification, the tool should be able to execute commands (like running a test suite, or a specific script) and capture the output. Amplifier’s _Parallel Worktree System_ suggests it spins up separate git worktrees where code can be run safely. For our first pass, we can simplify: create a temporary directory (or use the user’s project if safe) to run tests. For example, if it’s a Python project with pytest, the tool can call pytest and capture results.
*   We must be mindful of security and environment differences. Since this is a dev tool, we assume users run it in a controlled environment. Still, ideally run untrusted code in a restricted sandbox (perhaps use subprocess with timeouts, and clearly log what’s being executed).
*   The orchestrator can invoke this tool after the code generation step. If tests fail, it can feed the output (error logs, tracebacks) back to the AI (via the Debug agent) for analysis.
*   **Web Search / Fetch (Optional):** If the knowledge base is insufficient, sometimes the AI might need external info (e.g., search the web or fetch documentation from a URL). Amplifier lists WebSearch and WebFetch tools for agents[\[23\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=requires%20the%20api,Read%2C%20WebFetch%2C%20TodoWrite%2C%20WebSearch%2C%20BashOutput). We might defer these in the first iteration (since they add complexity and potential unpredictability). Instead, ensure our content ingestion is robust so common references are local. We can note this as a future addition.
*   **Validation Tools:** These include linters, static analyzers, or custom quality checks:
*   We saw Amplifier has _quality checks_ as an automation tool. For example, they likely enforce certain patterns or run a linter automatically. We can incorporate a simple version: e.g., run flake8 or a type checker on generated code as part of verification. Or use a custom rule like the _SycophancyDetector_ (Amplifier’s test for overly sycophantic replies) to ensure the AI’s style stays on track[\[24\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=%3Cfile%20path%3D,agent%20responses%20for%20sycophantic%20behavior)[\[25\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=return%20%7B%20,not%20is_sycophantic%20or%20contains_disagreement%2C). While not critical to functionality, including at least a placeholder for style/quality checks is good practice.
*   For now, perhaps implement a basic **AI response validator**: after each agent response, run checks like:
    *   If the agent was supposed to output JSON (like concept-extractor does in Amplifier[\[26\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Output%20Format)), validate it’s proper JSON.
    *   If the agent should not use certain phrases (e.g., avoid overly agreeing with the user on a bad idea), flag that (Amplifier’s anti-sycophancy patterns could be reused[\[27\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=SYCOPHANTIC_PATTERNS%20%3D%20%5B%20,that%5B%27%27%27%5Ds%20%28exactly%7Cprecisely%29%20right)).
    *   If code is output, maybe verify it compiles or at least is not missing brackets.
*   These validations can be logged or used by orchestrator to prompt the AI to fix its output if needed (“Your output had X issue, please correct it”).
*   **Tool Invocation Mechanism:** How will agents use these tools in practice? Two approaches:
*   _Orchestrator-managed:_ The orchestrator decides when to use a tool (e.g., after code generation, orchestrator triggers the Test run tool automatically).
*   _Agent-invoked:_ The agent’s AI output could contain a special command that indicates a tool usage, which the system intercepts, executes, and then returns the result to the AI. For example, the AI might output something like <ACTION>: RUN\_TESTS or a JSON block requesting a file read. Amplifier’s documentation hints at using a “Task tool to launch an agent”[\[28\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=designer%20agent%20to%20design%20a,example%3E%5CnContext%3A%20The%20user%20is) or the agent saying “Let me use X agent” in its response[\[29\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=assistant%3A%20,Context%3A%20The%20user%20wants%20to), which presumably causes the system to switch context. Setting up a full parsing loop for this is complex, so for the first version, **we will primarily use the orchestrator-managed approach** (explicit step in recipes for tools).
*   However, we can implement a **simple parser for known patterns** as a proof of concept. If, for instance, the AI (in analysis mode) says “I'll use the content-researcher agent to find relevant content”, we could intercept that (since we expected it) and actually perform a content search, then feed back results. This kind of hook or callback system could be part of the **hooks** the user mentioned. We’d define hooks on certain keywords or outputs to trigger tool functions.
*   **Extensibility:** Design the tool interface such that adding a new tool is straightforward. Perhaps a registry or a config (like a mapping from tool name to a function). This way, as we progress, we can plug in more capabilities without altering core logic. (Amplifier likely has something similar given the variety of tools listed.)

In summary, equipping the SDK with essential tools will empower the micro-agents to do more than just talk – they can act on the environment and verify outcomes. This is crucial for robust performance, as it closes the loop on tasks (especially coding tasks where you need to compile/run to truly validate). It also enforces the _automation and quality control_ aspect that Amplifier emphasizes.

### 6\. **Configuration and Project Structure**

To truly **“lean into the tool as a full encapsulation”**, we must ensure the new environment has its own configuration, context setup, and can be packaged separately. Important considerations:

*   **Environment Config**: Create a configuration system for the SDK/tool. Using Python’s Pydantic BaseSettings (as Amplifier does) is a good option for managing config via environment variables or a config file[\[30\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=class%20MemoryExtractionConfig%28BaseSettings%29%3A%20,system%20with%20environment%20variable%20support). We should have:
*   .env.example in the repo, containing keys like DATA\_DIR, CONTENT\_DIRS, maybe API\_KEY (if not using Claude CLI), etc., as placeholders. The dev team can copy this to .env for their environment. (Amplifier’s example .env shows how to set data and content directories.)
*   Config classes (e.g., ToolConfig) that load these values. This makes it easy to adjust paths or settings without code changes.
*   Possibly, config for selecting which model or API to use (e.g., MODEL\_PROVIDER=claude vs MODEL\_PROVIDER=openai). For now, default to Claude.
*   **Repository Structure**: Organize the code logically to separate concerns:
*   A possible layout:
    *   core/ or sdk/: containing the core modules (orchestrator, model interface, memory, tools, etc.).
    *   agents/: containing agent definition files (md or yaml).
    *   recipes/ (optional): definitions of task sequences if we externalize them.
    *   examples/ or scripts/: a folder for example use-cases or integration tests.
    *   tests/: for automated tests of the SDK components.
    *   We should also include a README documenting how to install and use this new tool, similar to Amplifier’s README but specific to this project.
*   **Separate Repos**: We anticipate possibly splitting into multiple repositories:
    *   **Repo A: Claude Code SDK (Core)** – a library that includes all core functionality (agents, orchestrator, etc.). This could be open-sourced or shared independently.
    *   **Repo B: Example Tool Implementation** – a thin wrapper or CLI that uses the SDK to actually perform tasks, potentially including any project-specific agents or configs. For instance, if we want to integrate this back into Amplifier or another product, that integration could live separately, calling into the SDK.
    *   For the first pass, it might be simplest to keep everything in one repository to reduce overhead. However, we will design the code in a modular way so that if we decide to factor out a separate library later, it’s straightforward (e.g. avoid hard-coding assumptions about a specific project).
    *   The key reason for separate repos is **isolation of resources**: no accidental dependency on Amplifier’s code or data. Our tool should run on its own. This also lets development progress without needing Amplifier’s heavy setup.
*   **Context and Hooks**: Ensure the new environment **initializes Claude with the right context** automatically, similar to how Amplifier does by launching Claude in the project directory. In our case, when the SDK is invoked (say via a CLI command like python run\_tool.py or similar), it should load:
*   All agent profiles into memory or into an initial prompt buffer.
*   Pre-defined system messages (like the design principles or any global instructions).
*   Possibly, load recent memory or relevant knowledge snippets if a user project is attached.
*   Essentially, mimic the “pre-loaded context” Amplifier boasts – so that from the first user prompt, the AI is already guided by our patterns.
*   **Hooks**: The system should allow customization or insertion of behavior at key points:
*   For example, a **pre-agent hook** (to modify or augment the prompt before sending to AI), or **post-agent hook** (to process the AI’s output before continuing). This will be useful as we integrate with other systems or add more complexity. We can implement this via simple callbacks or by designing the orchestrator to call overridable methods at these points.
*   Another hook is the _tool invocation_ as discussed – essentially intercepting certain agent outputs. We should keep that logic modular, perhaps in the agent interface or orchestrator, not tangled in the core loop.
*   These hooks/configurability points will make the SDK flexible for different use cases (and easier to integrate back into Amplifier if needed).

By setting up a clear config and project structure, we ensure that the dev team (and future contributors) have an easy time understanding and extending the system. It also ensures the solution is **self-contained** – you can spin it up in a fresh environment and it will bring along everything needed (from context definitions to execution logic).

## Step-by-Step Development Plan

To help the team tackle the implementation, here’s a suggested plan of action broken into steps:

1.  **Repository Setup:**
2.  Create a new repository (or two, if splitting core vs usage). Initialize it with a basic Python project structure. Include a README.md summarizing what this tool is and how to get started (you can borrow text from this doc for that).
3.  Set up a virtual environment and required dependencies (Python 3.11+ as in Amplifier, plus any libraries we know we’ll use: e.g., anthropic SDK or Claude CLI, sentence-transformers for embeddings (optional), pytest for testing our tool, etc.). Document these in a requirements.txt or pyproject.toml.
4.  Add a .env.example file with placeholders for configuration (data directory, content directories, API keys, etc.).
5.  **Implement Core Modules:**
6.  **LLM Interface Module:** Build a class (e.g. ClaudeClient or LLMClient) that encapsulates sending prompts and receiving responses. For now, it can have a simple method like complete(prompt: str) -> str. Internally, call the Claude API. If using the CLI, this class might invoke a subprocess for claude command. Ensure it reads any needed API keys from env if not using CLI. Test this module standalone with a simple prompt to verify connectivity.
7.  **Agent Definition Loader:** Write a loader that reads agent profile files from the agents/ directory. Perhaps all agent files are markdown with a frontmatter. We can parse them to extract at least the name and role description. For simplicity, we might hardcode a few agent profiles in code for now, and move them to external files later. The main goal is to have a dictionary of agent configurations we can refer to by name.
8.  **Knowledge/Memory Module:** Implement the MemoryStore and search functionality:
    *   MemoryStore: a simple class to save and retrieve text “memories.” Use a JSON file in DATA\_DIR (like memories.json). Use a structure similar to Amplifier’s (each memory with an ID, content, metadata). Provide methods: add\_memory(content, metadata), get\_memories() etc.
    *   Memory/Knowledge Searcher: integrate an embedding model. Install sentence-transformers and load all-MiniLM-L6-v2 (as Amplifier does[\[15\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Relevance%20scoring)). Provide a method search\_documents(query) -> list of (doc, score). If performance is an issue or to simplify, implement a keyword search fallback (e.g., simple substring or fuzzy matching across documents)[\[31\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L21222%20,Confidence%20scoring%20on%20edges). You can reuse parts of Amplifier’s search code – e.g., their approach to load/store embeddings in a JSON file[\[32\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L32503%20self,self.embeddings%20%3D%20self._load_embeddings)[\[33\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=def%20_save_embeddings%28self%29%3A%20,try%3A%20import%20json).
    *   Also implement a basic add\_document\_to\_index(doc\_path) that reads a file and stores its embedding (so we can build the index initially).
9.  **Tools Module:** Provide functions for at least:
    *   run\_tests() – which executes tests in the current project. Perhaps for now just call a shell command like pytest (or a specific command configured by env). Capture output and return it.
    *   execute\_code(code\_string) – (optional) to run arbitrary code snippets in a sandbox. This might be complex (could use exec() for Python or spin up a docker container for other languages). For first iteration, focus on running existing test suites rather than free-form execution.
    *   read\_file(path) and write\_file(path, content) – with safety checks (only allow paths within the project or a temp directory).
    *   Ensure these functions return their results in a structured way (so the orchestrator/agent can use them). Possibly wrap results in markdown or a code block if sending back into AI prompt (to maintain clarity).
10.  **Orchestrator (Task Manager) Module:** This is the most critical piece to implement carefully:
    *   Define a class Orchestrator or TaskManager that can run a _Recipe_. A recipe could be represented as a list of steps, where each step has: an agent to invoke (or a tool to run), and instructions on what to do.
    *   For initial simplicity, encode the recipe logic in code (if/else or sequence of calls). For example, implement a method handle\_user\_request(request: str) that does:
    *   If the request is recognized as a “code generation task” (maybe via a keyword or just default for now), then:
        *   Call Planner agent: compose prompt with Planner profile + user request → get plan.
        *   Save plan (in memory store and also keep in a local variable).
        *   Call Coder agent: compose prompt with Coder profile + (plan + user request or updated instructions) → get code output.
        *   Save the code (maybe write to a file or keep as string).
        *   Call test tool: run tests → get results.
        *   If tests passed, great. If failed:
            *   Call Debug agent (or reuse Coder with context of errors): prompt with error log + original code → get revised code.
            *   Loop back to test until pass or a max number of iterations.
        *   Finally, return or present the final code (and maybe summary of what was done).
    *   If the request is an “analysis task” (e.g. “Analyze documents…”), then:
        *   Call Analysis agent in TRIAGE mode: feed it perhaps a list of document titles or a summary of each (which we can prepare via knowledge search).
        *   Based on output, pick top docs → call Analysis agent in DEEP mode for each or combined.
        *   Then call Synthesis agent to merge insights into answer.
        *   (This is more complex; we might not implement in first pass unless needed. But structure the orchestrator so adding this flow later is possible.)
    *   The orchestrator should be instrumented with logging at each step for transparency (so developers can see what’s happening, and users get transparency as Amplifier values[\[34\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,in%20what%20AI%20is%20doing)).
    *   Also consider a **timeout or safety**: if an agent’s output is not helpful or goes off track, the orchestrator could detect that (via validation tools) and decide to retry or fail gracefully.
    *   Write basic unit tests for orchestrator logic using dummy functions or a stubbed LLM (to simulate, say, that the coder returns some code, etc.). This will ensure the sequence flows as expected.
11.  **Agent Output Validation:** Implement the SycophancyDetector or similar as an optional check on agent outputs (from Amplifier’s tests[\[35\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=SYCOPHANTIC_PATTERNS%20%3D%20%5B%20,i%20%28completely%7Ctotally%7Cabsolutely%29%20agree)). This can be integrated after each step – e.g., warn if the assistant is just agreeing with user without actual substance. Also, ensure that if an agent was supposed to output JSON or code, it did so correctly (could attempt to parse it to see if valid). For the first iteration, logging these issues might suffice (no need to auto-correct them yet, but at least we know).
12.  **Integrate Components:**
13.  Connect the orchestrator with the LLM interface: ensure the orchestrator calls ClaudeClient.complete(prompt) with the right prompt structure. Develop a consistent formatting for prompts:
    *   Possibly use a system message for agent role, and user message for task, etc., depending on the API’s support. Claude might not use the exact ChatGPT style roles, but you can simulate by prefixing the agent description.
    *   Make sure multi-line prompts, code blocks, etc., are handled properly (the CLI or API might need specific parameters or formatting).
14.  Connect memory and knowledge: before calling the LLM for a given step, the orchestrator can fetch relevant knowledge snippets and append them to the prompt (or provide as a reference section). For example, if the Planner agent is to create a plan, and we have relevant design guidelines in the knowledge base, supply those.
15.  Connect tools: incorporate tool usage into the orchestrator flow as planned. E.g., after getting code from Coder, call the run\_tests() tool and capture output.
16.  Load config from .env: use the config to set where the data and content directories are, load any content files into the knowledge index at startup.
17.  Also, implement a simple CLI entry-point (maybe a main() in run\_tool.py) that reads user input (or takes a user prompt as an argument) and feeds it to the orchestrator, then prints the final result. This helps to manually test the end-to-end flow in this first pass.
18.  **Testing & Iteration:**
19.  Start by testing a simple scenario manually: e.g., give a trivial coding task (“write a hello world function”) and see if the orchestrator goes through plan, code, test properly. Observe the outputs at each stage and adjust prompt wording or logic as needed.
20.  Write unit tests for individual modules where feasible (memory search returns expected docs, file read/write works, etc.). Also consider integration tests for a recipe (perhaps simulate that the LLM always returns a certain answer, using a stub, to test the loop).
21.  Verify that the system indeed leverages the agent profiles (e.g., if you intentionally provide a “bad idea” to the Planner, does it follow the philosophy and not just agree blindly? If not, maybe enhance the prompt or add the anti-sycophancy check to force a better response).
22.  Make sure the environment variables and configuration are effective – test pointing CONTENT\_DIRS to a folder of sample docs and see if the knowledge search works.
23.  Test the “encapsulation”: try running this tool outside the Amplifier repo entirely, with just the new repository, to ensure nothing accidentally depends on Amplifier’s presence. The new project should carry all needed assets (agent definitions, etc.) internally.
24.  **Documentation & Handoff:**
25.  Document each agent profile (what it’s for) and each tool in the README or a separate docs file, so the dev team and future maintainers know how to add new ones.
26.  Also document the recipe flows implemented. For example, provide a section “**Implemented Microtask Workflows**” listing the code-generation pipeline steps, and (if applicable) the analysis pipeline steps. This guides users in what to expect and helps developers see the structure for adding more.
27.  If there are any limitations or things not done in this pass, note them (e.g., “Web search not yet integrated; to be added later” or “Parallel solution exploration not implemented in v1”).
28.  Ensure to outline the philosophy of usage: e.g., when to use which agent or mode. The team should understand _why_ to break tasks in certain ways. For instance, mention: _“For broad analysis (many documents), always start with TRIAGE to narrow scope_[_\[22\]_](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Initial%20Request%3A%20,SYNTHESIS%20mode%20to%20combine%20findings)_. For any coding task that affects running software, always run tests to verify (automation principle). Use specialized agents whenever the task clearly falls into their domain – e.g., use the Debug agent if the primary need is fixing a failing test, use the Security agent if the question is about hardening code, etc.”_ These guidelines can be part of the developer documentation or even encoded as checks in orchestrator (like choosing agent based on keywords).

Following this plan, the small dev team can incrementally build a working prototype of the Claude Code microtask SDK. The emphasis is on **getting a functional workflow in place using Amplifier’s best practices**, rather than covering every edge case. We want a demonstration that _Claude Code can be a “force multiplier” with minimal guidance by leveraging context and structured workflows_[\[4\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,the%20AI%20through%20complex%20tasks) – even outside the full Amplifier environment.

## Example Workflow: Code Generation Microtask Pipeline

To illustrate how all these pieces come together, let’s walk through an example use-case that our first-pass implementation should handle. This will clarify the interactions between components:

**User’s Request:** _“Add a feature to calculate the factorial of a number in our project. Include unit tests.”_

**Step 1 – Planning (Architecture Agent):**
The orchestrator recognizes a coding task and invokes the Planner/Architecture agent. - _Context provided:_ The user request, possibly a snippet of the relevant project context (if available, e.g., “the project is a simple math library”), and the Planner agent’s role instructions (e.g., “You are a software architect. Plan the steps to implement the requested feature, considering any design patterns or existing code.”). - _AI (Planner agent) response:_ A structured plan. For example: 1. Identify where the code should reside (e.g., a new function math\_utils.factorial(n)). 2. Outline the logic for factorial (iterative or recursive). 3. Write unit tests for base cases and a typical case. 4. Ensure error handling for negative input. - _Post-processing:_ Orchestrator may store this plan in memory (with an ID, so it can be recalled) and log it.

_(If the plan is unsatisfactory or incomplete, the orchestrator could prompt the planner again or adjust instructions, but in first pass we might assume a decent plan on first try.)_

**Step 2 – Implementation (Coder Agent):**
Now the orchestrator engages the Coder agent to implement the plan. - _Context provided:_ The plan from step 1, the user’s original request, any template or coding standards from knowledge base (for instance, if the project has a coding style guide in content, provide that), and Coder agent instructions (“You are an expert Python developer. Write the code to implement this feature following the plan. Provide the code only, no explanation.”). - _AI (Coder) response:_ The agent returns code, for example:

def factorial(n: int) -> int:
"""Calculate factorial of a number."""
if n < 0:
raise ValueError("Factorial is not defined for negative numbers")
return 1 if n in (0, 1) else n \* factorial(n-1)

along with maybe a test in a string (if asked in one go) or separate. Let’s say it outputs just the function (our design might be to handle tests separately). - _Post-processing:_ The orchestrator captures the code output. It might save this to a file in a temp directory, e.g., math\_utils.py, to prepare for testing.

**Step 3 – Testing (Tool + possibly Test Agent):**
Orchestrator now runs tests to validate the code. - _If tests were already written in the project_: just run pytest via the tool. If not, we might have an agent to create tests. In this example, the user explicitly said “include unit tests,” so we might have the agent generate a test file as well. Alternatively, the orchestrator could have instructed the coder to output tests too. There are design choices: - **Option A:** Coder agent also outputs tests (maybe in a second code block). Then orchestrator saves those to test\_math\_utils.py. - **Option B (microtask approach):** Have a separate microtask where a **Tester agent** (or reuse the coder with a testing prompt) generates the tests. This might yield better structured results. Let’s assume we do this: - Orchestrator calls Tester agent with context: “The implementation is done (show code). Now write a unit test function for it.” Tester agent profile might say “You are a testing expert, write thorough unit tests.” - AI returns tests, e.g.:

def test\_factorial():
assert factorial(0) == 1
assert factorial(1) == 1
assert factorial(5) == 120
with pytest.raises(ValueError):
factorial(-3)

\- Orchestrator saves this as test\_math\_utils.py. - Now orchestrator uses the **Run Tests tool**. It executes pytest in the temp environment. - _Tool output:_ Suppose one test fails (just to exercise the loop). Maybe our implementation had a bug. The output says: “Test failed: maximum recursion depth exceeded” (for a large n, if not tail-optimized, but our small tests might pass – let’s imagine a failure anyway). - Orchestrator captures the failure output and logs it.

**Step 4 – Debugging (Debug Agent):**
Seeing tests failed, orchestrator triggers the Debug agent. - _Context:_ Provide the Debug agent with the code and the error trace from tests. Debug agent’s instruction might be: “You are a debugging specialist. Analyze why the test failed and suggest a fix.” - _AI (Debug) response:_ Perhaps it spots that the recursive call might be fine for small n but could hit recursion limit for larger n, or maybe a logic mistake. It suggests a fix, e.g., switching to an iterative approach for safety. - Or, if it’s a simple logic error, it directly provides a corrected code snippet. - _Post-processing:_ Orchestrator applies the fix (if code provided, replace the function in file, or instruct coder agent again to implement the suggestion).

**Step 5 – Re-run Tests:**
Orchestrator runs tests again via the tool. - This time, all tests pass. - Orchestrator proceeds to finalization.

**Step 6 – Completion:**
The orchestrator now has a verified solution. It can assemble the final output for the user: - Perhaps present the final code (and possibly the tests) to the user, or if integrated in an IDE, it might have already written them to the project. - It could also provide a brief summary: “Implemented factorial in math\_utils.py and all tests are passing.”

Throughout this flow, the **design principles and philosophies** we carried from Amplifier are applied: - We used specialized roles (architect, coder, tester, debugger) at each phase to leverage focused expertise. - We loaded _existing knowledge_ (style guides, etc.) when needed, rather than relying on the model’s memory alone[\[6\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=The%20value%20of%20Amplifier%20isn%27t,specific%20AI%20%E2%80%94%20it%27s%20in). - We maintained a _human-AI partnership mindset_: the user’s intent guided the tasks, and the AI was transparent about what it was doing at each step (the orchestrator could output logs like “Planning solution…”, “Coding…”, “Running tests…” so the user is not in the dark). - We enforced _automation for quality_: tests were run automatically, and a bug was caught and fixed with minimal human intervention, demonstrating that the AI can handle iterative improvement (Amplifier’s “minimal guidance” ideal[\[4\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,the%20AI%20through%20complex%20tasks)). - We kept components modular: each agent’s logic was under a focused 150-line (hypothetically) implementation, aligning with the _modular architecture_ goal[\[2\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Modular%20Architecture).

This end-to-end example is exactly what the first-pass implementation should achieve for at least one scenario. It shows the power of microtask orchestration: more dependable outcomes and clarity in process.

## Guidelines on “Which Approach to Use When”

As we build and use this microtask SDK, it’s important to articulate _when to apply certain agents or patterns_, ensuring the system is robust and chooses the right strategy. Here we outline the philosophy behind selecting modes, agents, or tools for different situations – effectively encoding the “which to use for what” knowledge for the dev team:

*   **Monolithic vs. Stepwise:** If a user request is straightforward and self-contained (e.g., “fix typo in output message”), a single-step direct answer from Claude might suffice. However, as soon as a task involves multiple distinct aspects (e.g., _understand requirement, write code, verify correctness_), the orchestrator should break it down. **Rule of Thumb:** When in doubt, prefer breaking a task into smaller microtasks – it’s easier to combine correct sub-results than to untangle a large confused monologue from the AI.
*   **Use Specialized Agents Proactively:** Don’t wait for the AI to struggle. If the request clearly aligns with a known agent’s expertise, use that agent from the start. For example:
*   For architectural guidance or high-level planning, invoke the **Architect/Planner agent** before any coding. This ensures a solid blueprint and reduces backtracking.
*   For debugging tasks, go straight to the **Debug agent** rather than using the generic coder. The debug agent is more likely to systematically analyze errors and handle the “fix” mindset.
*   If the query relates to a specific domain (security, performance, etc.) and we have an agent for it, leverage that. (In future, we might have a **Security Auditor agent** for security review tasks, etc.)
*   Amplifier’s approach was to **“use \[specialized\] agents proactively for any analysis task – the engine will select the optimal mode”**[\[36\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L3396%20,engine%20agent). We mirror this by hard-coding those selections for now (since we don’t have an AI “engine” making meta-decisions yet).
*   **Mode Selection for Analysis:**
*   If dealing with **large volumes of information** (many documents or a big knowledge base query), start in **TRIAGE mode** – i.e., use a quick filtering strategy. This might mean instructing an agent to list top relevant sources or using the search tool to narrow scope[\[3\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=%F0%9F%94%8D%20TRIAGE%20Mode%20,Relevance%20Filtering). _When to triage:_ user asks broad questions like “Analyze these 50 documents…” or “What’s the trend in this 100-page report?”.
*   Move to **DEEP mode** when detail is needed on a subset. E.g., after triage picks 3 important docs, have the agent fully read and analyze those. _When to deep-dive:_ when high relevance info is identified or the question is very specific to certain sources.
*   Use **SYNTHESIS mode** to compile insights from multiple sources into a coherent answer. _When to synthesize:_ any time multiple inputs (documents, perspectives) need combining, especially after deep analysis of each[\[22\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Initial%20Request%3A%20,SYNTHESIS%20mode%20to%20combine%20findings). The Synthesis agent should produce a well-structured answer, citing sources or reconciling differences as needed.
*   These modes can be implemented as separate steps/agents or as a parameter to a single Analysis agent. Initially, it might be easiest to treat them as separate agents (even if they share logic).
*   **When to Consult the Knowledge Base:**
*   Always query the knowledge base if the user’s query involves any factual information or reference to something that might be documented. This helps avoid hallucinations and ensures accurate answers. For example, if the user asks, “Implement function X according to the spec,” and we have that spec in the content directory, the orchestrator should fetch it and provide it to the agent.
*   If the AI’s response would benefit from concrete examples or definitions (like implementing a standard algorithm, or using an API), searching the knowledge base for those terms can provide confirmation or templates. Our system can either feed that info into the prompt or even provide it directly to the user as reference.
*   **Philosophy:** Rely on explicit knowledge rather than the model’s implicit knowledge whenever possible[\[6\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=The%20value%20of%20Amplifier%20isn%27t,specific%20AI%20%E2%80%94%20it%27s%20in). The model might “know” what factorial is, but by giving it the context of a spec or a known implementation pattern, we reduce error and increase trust.
*   **Tool Use vs AI-Only:**
*   Use automated **tools** whenever the task involves mechanical verification or data retrieval that is straightforward. For instance, running tests or searching text are better done with tools (guaranteeing correct results) than asking the AI to imagine the output. Tools provide ground truth that the AI can then interpret.
*   Conversely, use the **AI’s reasoning** for tasks that require understanding, creativity, or synthesis. E.g., deciding how to fix a failing test is a reasoning task for the Debug agent, but getting the test failure message is a job for the test tool.
*   **Integration point:** The orchestrator sits at this interface – it decides when to call a tool and when to ask the AI. A good practice is: after any AI generation that produces something to be validated (code, plans, claims), use a tool to validate it (run code, check references, etc.). After any tool produces data (log output, search results), use an AI agent to analyze or act on it if interpretation is needed.
*   **Error Handling Strategy:**
*   If an AI output seems off or fails a check (e.g., code didn’t compile, or the answer contradicts known facts), don’t hesitate to loop back and tell the AI to correct itself. The system should not accept a result until basic validations pass. This might mean adding a subtask: e.g., after synthesis, maybe have a quick review agent verify no contradictions or glaring gaps (Amplifier future plans include contradiction detection[\[37\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L34800%20,file)).
*   In implementation, this translates to: **validate, then iterate**. If code fails tests, go to debug. If an answer seems incomplete, prompt a clarification or add another microtask (maybe “Reflection agent” to critique the answer before finalizing – we could incorporate that later).
*   **Human in the Loop (when to ask user):**
*   While the goal is minimal guidance, sometimes it’s important to clarify with the user. Our microtask system can include a step for clarification if the initial request is too ambiguous. For example, a **Clarification agent** (or simply the orchestrator asking) could summarize the understanding and ask the user to confirm.
*   This should be used sparingly and only when the risk of proceeding incorrectly is high. (We mention this for completeness, though initial implementation might skip explicit user clarification steps unless absolutely needed.)
*   Example: User says “Add logging to the service.” This is open-ended. The system might respond with a quick question: “I can add logging. Do you want verbose debug logging or just errors?” (Better to clarify than implement wrongly.)

By codifying these guidelines, the development team can implement logic in the orchestrator and agent behaviors that align with them. It might be as simple as comments in code (“# If more than 5 docs, use triage mode”) or as explicit as writing functions that detect scenario types. Over time, these philosophies can even be learned by the AI itself (the agents can be instructed to decide modes), but initially, the team will hard-code these choices based on this understanding.

## Future Extensions and Next Steps

Once the first-pass system is up and running, we’ll have a baseline to evaluate and build upon. There are some features from Amplifier and the overall vision that we intentionally scoped out of the initial implementation but should keep in mind:

*   **Recipe Definition Language:** As noted in Amplifier’s Phase 2 plans, eventually we might want a more declarative way to specify recipes (microtask sequences)[\[1\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Recipe%20sharing%20and%20discovery). In the future, we could create a YAML or domain-specific language to define new workflows without writing new Python code. For now, our orchestrator logic is imperative; but we should structure it in a way that it could read from a data-driven recipe (e.g., a JSON file listing steps and agents).
*   **Parallel Workstreams:** The current design is sequential for simplicity. Later, we can explore parallelizing certain tasks – for example, trying two different implementations simultaneously and comparing (Amplifier’s _parallel worktree_ concept). Our SDK could allow spawning multiple agent instances in parallel, if the model API permits concurrent calls. In first pass, keep things synchronous; but keep an eye on where parallelism could slot in (maybe design the orchestrator to not assume global state that would break if two flows ran at once).
*   **Learning and Improvement:** Amplifier emphasized “learn from every interaction” and compounding knowledge[\[38\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Share%20recipes%20for%20common%20workflows)[\[6\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=The%20value%20of%20Amplifier%20isn%27t,specific%20AI%20%E2%80%94%20it%27s%20in). We have basic memory storage, but we can enhance this:
*   Implement a mechanism to automatically extract key _learnings_ after a task completes (similar to Amplifier’s MemoryExtractor which parses conversation logs for insights[\[21\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=class%20MemoryExtractor%3A%20,conversation%20text)). For example, after a coding task, store a memory like “Implemented factorial using recursion, noted recursion depth consideration.” Such memories could inform future decisions (like next time a factorial or recursion comes up, the system might recall this).
*   Also, track patterns: if the AI tends to make a certain mistake and we fix it, record that pattern so the AI/agent can avoid it next time (this could be a simple list of “gotchas” to incorporate into prompts).
*   **User Interface & Integration:** In the first pass, interaction might be via CLI. Eventually, this could be integrated into IDEs (VS Code extension, etc.), or into Amplifier as a plugin. The design of being separate should facilitate that. We might create an API for the SDK (so other tools can call orchestrator.handle\_request(request)) to embed this functionality.
*   **Model Agnosticism:** Currently we use Claude; but we should monitor if OpenAI or others have superior coding models. Our abstraction will allow swapping. We might test the SDK with GPT-4 or other models to compare outcomes. If switching, ensure the agent prompts are compatible or adjust accordingly.
*   **Expand Agent Portfolio:** Add more specialized agents as needed:
*   E.g., _Security auditor_, _Performance optimizer_, _UX reviewer_, etc., each with its own heuristics and tools. Amplifier had many such as _database-architect_, _insight-synthesizer_, _integration-specialist_, etc., as seen in their examples[\[39\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L5755%20database%20performance,architect)[\[40\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L7100%20description%3A%20Expert,servers%2C%20handling%20API%20integrations%2C%20or). We likely won’t implement these immediately, but our framework will allow it. We can gradually port more of those agent definitions from Amplifier’s repo into our agents/ directory, enabling the orchestrator to call them when relevant.
*   **Advanced Validation and Reasoning:** Incorporate things like:
*   _Contradiction detection_ in outputs (especially for research answers, ensure the final synthesis doesn’t internally conflict).
*   _Confidence estimation_: Agents could output a confidence level for their answers. The orchestrator can decide to maybe double-check a low-confidence answer (maybe automatically do a quick web search or alternate approach to verify).
*   _Multi-model ensemble_: In future, for critical tasks we could query multiple models (Claude, GPT, etc.) and compare answers (majority vote or have one verify the other’s output).
*   **Recipe Sharing and Discovery:** A longer-term idea (from Amplifier Phase 2)[\[1\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Recipe%20sharing%20and%20discovery) is to let users or the community define new recipes and share them. This suggests our recipe format (once in place) should be standardized and we could build a repository of useful microtask workflows (like “refactor code”, “generate documentation from code”, etc.). This is beyond first-pass but something to design for.

For now, these are just notes for the roadmap. The first-pass implementation, as detailed above, lays the groundwork such that these enhancements can plug in naturally. Each component we build should be done with an eye toward these future needs (e.g., keep the orchestrator flexible for new steps, keep the model interface flexible for new models, etc.).

**In conclusion**, this complementary document provides the development team with a concrete plan to implement the initial version of the Claude Code SDK with a microtask-driven approach. By following this guide – which covers architecture, key components to replicate from Amplifier, detailed implementation steps, and the underlying philosophy – the team should be well-equipped to start building. The result will be a self-contained tool (in a new repo) that demonstrates how structured orchestration of Claude (or other LLMs) can dramatically improve the reliability and depth of AI-assisted development workflows. It’s an exciting step that takes the proven ideas from Amplifier and distills them into a focused toolkit that others can use and build upon.

With this plan in hand, the team can now proceed to **“take a swing” at implementing the first pass** – confident that they are carrying forward the best of Amplifier’s innovations into a new, robust SDK. Good luck, and happy building!

**Sources:**

*   Amplifier project documentation and design notes[\[1\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Recipe%20sharing%20and%20discovery)[\[6\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=The%20value%20of%20Amplifier%20isn%27t,specific%20AI%20%E2%80%94%20it%27s%20in) (for reference on features and goals)
*   Amplifier agents and modes examples[\[3\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=%F0%9F%94%8D%20TRIAGE%20Mode%20,Relevance%20Filtering)[\[9\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=requires%20the%20api,Read%2C%20WebFetch%2C%20TodoWrite%2C%20WebSearch%2C%20BashOutput) (illustrating triage/deep analysis and specialized agent usage)
*   Amplifier code snippets (memory and search system)[\[14\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L34698%20,Relevance%20scoring)[\[19\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=memory_embeddings%20%3D%20self,type%3A%20ignore) (for implementing knowledge integration and memory search)

[\[1\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Recipe%20sharing%20and%20discovery) [\[2\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Modular%20Architecture) [\[3\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=%F0%9F%94%8D%20TRIAGE%20Mode%20,Relevance%20Filtering) [\[4\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,the%20AI%20through%20complex%20tasks) [\[5\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,to%20any%20particular%20AI%20technology) [\[6\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=The%20value%20of%20Amplifier%20isn%27t,specific%20AI%20%E2%80%94%20it%27s%20in) [\[7\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Ruthless%20Simplicity) [\[8\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L4001%20%3Cfile%20path%3D%22.claude%2Fagents%2Fapi,design%2C%20review%2C%20or%20refactor%20API) [\[9\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=requires%20the%20api,Read%2C%20WebFetch%2C%20TodoWrite%2C%20WebSearch%2C%20BashOutput) [\[10\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L5395%20,researcher%20agent%20to%20analyze%20our) [\[11\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=You%20are%20an%20API%20contract,needs%20rather%20than%20hypothetical%20futures) [\[12\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L10167%203.%20,maintenance%20cost%20of%20complex%20optimizations) [\[13\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=database%20performance%20issues.%5Cnuser%3A%20,architect) [\[14\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L34698%20,Relevance%20scoring) [\[15\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Relevance%20scoring) [\[16\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Confidence%20scoring%20on%20edges) [\[17\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,search%20if%20embedding%20fails) [\[18\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=query_embedding%20%3D%20self,type%3A%20ignore) [\[19\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=memory_embeddings%20%3D%20self,type%3A%20ignore) [\[20\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=class%20MemoryStore%3A%20%22%22%22JSON,with%20rotation%20and%20compatibility) [\[21\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=class%20MemoryExtractor%3A%20,conversation%20text) [\[22\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Initial%20Request%3A%20,SYNTHESIS%20mode%20to%20combine%20findings) [\[23\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=requires%20the%20api,Read%2C%20WebFetch%2C%20TodoWrite%2C%20WebSearch%2C%20BashOutput) [\[24\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=%3Cfile%20path%3D,agent%20responses%20for%20sycophantic%20behavior) [\[25\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=return%20%7B%20,not%20is_sycophantic%20or%20contains_disagreement%2C) [\[26\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=Output%20Format) [\[27\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=SYCOPHANTIC_PATTERNS%20%3D%20%5B%20,that%5B%27%27%27%5Ds%20%28exactly%7Cprecisely%29%20right) [\[28\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=designer%20agent%20to%20design%20a,example%3E%5CnContext%3A%20The%20user%20is) [\[29\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=assistant%3A%20,Context%3A%20The%20user%20wants%20to) [\[30\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=class%20MemoryExtractionConfig%28BaseSettings%29%3A%20,system%20with%20environment%20variable%20support) [\[31\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L21222%20,Confidence%20scoring%20on%20edges) [\[32\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L32503%20self,self.embeddings%20%3D%20self._load_embeddings) [\[33\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=def%20_save_embeddings%28self%29%3A%20,try%3A%20import%20json) [\[34\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,in%20what%20AI%20is%20doing) [\[35\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=SYCOPHANTIC_PATTERNS%20%3D%20%5B%20,i%20%28completely%7Ctotally%7Cabsolutely%29%20agree) [\[36\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L3396%20,engine%20agent) [\[37\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L34800%20,file) [\[38\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=,Share%20recipes%20for%20common%20workflows) [\[39\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L5755%20database%20performance,architect) [\[40\]](file://file-Majzcip3itFMmu6zyPi9im#:~:text=match%20at%20L7100%20description%3A%20Expert,servers%2C%20handling%20API%20integrations%2C%20or) repomix-output-microsoft-amplifier-2025-09-15-12-00-partial.xml

[file://file-Majzcip3itFMmu6zyPi9im](file://file-Majzcip3itFMmu6zyPi9im)